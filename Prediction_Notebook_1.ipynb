{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Competition:\n",
        "Predict the House Prices using advanced regression techniques\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Ug5bWTvl3Qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Packages\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aXlkTGxTZ8RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "JLDSBNzNmZkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import google drive library to load csv files:"
      ],
      "metadata": {
        "id": "X53VDg8ujtGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7yeZLcpUjsr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "fBee6pAOnebz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change path directory to redirect to data folder:"
      ],
      "metadata": {
        "id": "-FwZRdR_pVHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab_Notebooks/Kaggle/Price_Prediction\n",
        "!ls data"
      ],
      "metadata": {
        "id": "4MIXsPpJo1yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read datasets - train.csv and test.csv\n",
        "Datasets will be read from **google drive** path.\n",
        "Data path is set to '/MyDrive/Colab_Notebooks/Kaggle/Price_Prediction/data/'\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4HK863xPaBtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_path = '/MyDrive/Colab_Notebooks/Kaggle/Price_Prediction/data/'"
      ],
      "metadata": {
        "id": "HLACdXjkkZQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_8vRjABluuB"
      },
      "outputs": [],
      "source": [
        "# Read train.csv and test.csv\n",
        "train_df = pd.read_csv( 'data/train.csv');\n",
        "test_df = pd.read_csv('data/test.csv');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.describe())\n",
        "print(test_df.describe())"
      ],
      "metadata": {
        "id": "xrlPVqBEa5eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping the \"id\" column from the dataset:"
      ],
      "metadata": {
        "id": "GcpsiNDfahdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the ID columns\n",
        "train_df.drop('Id', axis = 1, inplace = True)\n",
        "test_df.drop('Id', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "rjMABU5p2WFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation and Heatmap\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kSPQChBddF8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corrMat = train_df.corr()\n",
        "corrMat"
      ],
      "metadata": {
        "id": "JthDp-80ZQFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "sns.heatmap(corrMat, vmax=1,square=True, cmap=\"YlGnBu\")\n",
        "plt.title(\"Correlation Matrix\")"
      ],
      "metadata": {
        "id": "kJZW3YZDdQvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dividing training dataset into x and y\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1W01YBxMhzAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# divide train data in x and y\n",
        "train_df_y = train_df['SalePrice']\n",
        "train_df_x = train_df.drop('SalePrice', axis=1)"
      ],
      "metadata": {
        "id": "gVWrkKgnh3dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess train.csv\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3Y3MAuUf2DAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing Missing Values in train.csv\n"
      ],
      "metadata": {
        "id": "7_taQoS90ig7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing NA values in LotFrontage column with the mean value of the column data\n",
        "meanValueLF = np.round(train_df_x['LotFrontage'].mean(), 2)\n",
        "train_df_x['LotFrontage'].fillna(meanValueLF, inplace = True);\n",
        "\n",
        "# Replace Alley column value NA with Unknown\n",
        "train_df_x['Alley'] = train_df_x['Alley'].replace(np.nan, 'Unknown')\n",
        "\n",
        "# Imputing NA values in MasVnrArea column with mode value\n",
        "train_df_x['MasVnrArea'].fillna(train_df_x['MasVnrArea'].mode(), inplace = True)\n",
        "train_df_x['MasVnrArea'] = train_df_x['MasVnrArea'].replace(np.nan, 0)\n",
        "\n",
        "# Dropping rows with NA values in GarageYrBlt\n",
        "train_df_x.dropna(subset=['GarageYrBlt'], inplace = True)\n",
        "\n",
        "#train_df_x.to_csv('testaru.csv')"
      ],
      "metadata": {
        "id": "mksbHpmQOeVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change \"object\" categorical columns into numerical categorical columns in train.csv\n"
      ],
      "metadata": {
        "id": "oZcyKjEPsNAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainObjCol = list(train_df_x.select_dtypes(include=['object']).columns)\n",
        "for c in trainObjCol:\n",
        "  train_df_x[c] = train_df_x[c].astype('category').cat.codes"
      ],
      "metadata": {
        "id": "b5yHYye8KhiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying standard scaler on train.csv"
      ],
      "metadata": {
        "id": "JWZg2Iu_iPNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scalar = StandardScaler()\n",
        "train_df_X = scalar.fit_transform(train_df_x)"
      ],
      "metadata": {
        "id": "VFGL8NYriT1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess test.csv\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Uk9iJMsL5Lj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing Missing Values in Test dataset"
      ],
      "metadata": {
        "id": "7BdEZVd8tKjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing NA values in LotFrontage column with the mean value of the column data\n",
        "meanValueLFtest = np.round(test_df['LotFrontage'].mean(), 2)\n",
        "test_df['LotFrontage'].fillna(meanValueLFtest, inplace = True);\n",
        "\n",
        "# Replace Alley column value NA with Unknown\n",
        "test_df['Alley'] = test_df['Alley'].replace(np.nan, 'Unknown')\n",
        "\n",
        "# Imputing NA values in MasVnrArea column with mode value\n",
        "test_df['MasVnrArea'].fillna(test_df['MasVnrArea'].mode(), inplace = True)\n",
        "train_df['MasVnrArea'] = train_df['MasVnrArea'].replace(np.nan, 0)\n",
        "\n",
        "# Dropping rows with NA values in GarageYrBlt\n",
        "test_df.dropna(subset=['GarageYrBlt'], inplace = True)"
      ],
      "metadata": {
        "id": "Nu7yVkyWtHDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change \"object\" categorical columns into numerical categorical columns in test.csv"
      ],
      "metadata": {
        "id": "qs_wndQK6yYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testObjCol = list(test_df.select_dtypes(include=['object']).columns)\n",
        "for c in testObjCol:\n",
        "  test_df[c] = test_df[c].astype('category').cat.codes"
      ],
      "metadata": {
        "id": "Rsww-0Av2XXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying standard scaler on test.csv"
      ],
      "metadata": {
        "id": "gVU7DOcS7Bk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scalar = StandardScaler()\n",
        "test_df_X = scalar.fit_transform(test_df)"
      ],
      "metadata": {
        "id": "Dqj8cnBd7Dnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principle Component Analysis\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6ZM33wCKsuAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca=PCA()\n",
        "pca_res = pca.fit_transform(train_df_X)\n",
        "pca_res"
      ],
      "metadata": {
        "id": "1HiEvO-4rGoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca.get_covariance()"
      ],
      "metadata": {
        "id": "zcYZ-US5tpJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explained_variance=pca.explained_variance_ratio_\n",
        "explained_variance"
      ],
      "metadata": {
        "id": "4AAxDOggt6is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting explained variance from PCA:"
      ],
      "metadata": {
        "id": "RnMW7_yWu3h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with plt.style.context('dark_background'):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.bar(range(79),explained_variance, alpha=0.5,align='center', label='individual explained variance' )\n",
        "    plt.xlabel('Principal components')\n",
        "    plt.ylabel('Explained variance ratio')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # range(79) used because number of features in the data is 79"
      ],
      "metadata": {
        "id": "vzAoAlldu70W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest contribution of the features to the variance is approximately 14%. We are going to consider only those features whose contribution is more than 2% to the variance. From the above bar chart, we will consider 10 components to be considered."
      ],
      "metadata": {
        "id": "hmGq_weiwV_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca=PCA(n_components=10)\n",
        "pca_res_new = pca.fit_transform(train_df_X)\n",
        "pca_res_new"
      ],
      "metadata": {
        "id": "PXPwxIMYxxQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca.get_covariance()"
      ],
      "metadata": {
        "id": "Ar86TginyBaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explained_variance_new=pca.explained_variance_ratio_\n",
        "explained_variance_new"
      ],
      "metadata": {
        "id": "eXpOGHO-yIsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with plt.style.context('dark_background'):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.bar(range(10),explained_variance_new, alpha=0.5,align='center', label='individual explained variance' )\n",
        "    plt.xlabel('Principal components')\n",
        "    plt.ylabel('Explained variance ratio')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "FZovC-fIySIj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}